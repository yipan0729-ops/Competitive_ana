"""
URL çˆ¬è™«æ¨¡å—ï¼ˆä¸‰å±‚ç­–ç•¥ï¼šFirecrawl â†’ Jina â†’ Playwrightï¼‰
"""
import os
import re
import hashlib
from typing import Dict, Optional, List, Tuple
from pathlib import Path
from datetime import datetime
import requests
from urllib.parse import urlparse

from src.config import config


class PlatformIdentifier:
    """å¹³å°è¯†åˆ«å™¨"""
    
    PLATFORMS = {
        "mp.weixin.qq.com": "å¾®ä¿¡å…¬ä¼—å·",
        "xiaohongshu.com": "å°çº¢ä¹¦",
        "xhslink.com": "å°çº¢ä¹¦",
        "zhihu.com": "çŸ¥ä¹",
        "douyin.com": "æŠ–éŸ³",
        "taobao.com": "æ·˜å®",
        "tmall.com": "å¤©çŒ«",
        "jd.com": "äº¬ä¸œ",
        "bilibili.com": "Bç«™",
    }
    
    @classmethod
    def identify(cls, url: str) -> Tuple[str, bool]:
        """
        è¯†åˆ«å¹³å°
        
        Returns:
            (å¹³å°åç§°, æ˜¯å¦éœ€è¦ç™»å½•)
        """
        domain = urlparse(url).netloc.lower()
        
        for pattern, platform in cls.PLATFORMS.items():
            if pattern in domain:
                needs_login = platform in ["å¾®ä¿¡å…¬ä¼—å·", "å°çº¢ä¹¦", "æ·˜å®", "å¤©çŒ«", "äº¬ä¸œ"]
                return platform, needs_login
        
        return "å®˜ç½‘", False


class URLCrawler:
    """URL çˆ¬è™«ï¼ˆä¸‰å±‚ç­–ç•¥ï¼‰"""
    
    def __init__(self):
        self.firecrawl_key = config.FIRECRAWL_API_KEY
        self.data_dir = config.DATA_DIR
    
    def crawl(self, url: str, competitor_name: str = "Unknown") -> Dict:
        """
        çˆ¬å– URL å†…å®¹
        
        Args:
            url: ç›®æ ‡ URL
            competitor_name: ç«å“åç§°
        
        Returns:
            {
                "success": bool,
                "content": str,  # Markdown å†…å®¹
                "content_path": str,  # ä¿å­˜è·¯å¾„
                "images": List[str],  # å›¾ç‰‡è·¯å¾„åˆ—è¡¨
                "metadata": dict
            }
        """
        print(f"ğŸ•·ï¸  çˆ¬å–: {url}")
        
        # è¯†åˆ«å¹³å°
        platform, needs_login = PlatformIdentifier.identify(url)
        print(f"   å¹³å°: {platform} | éœ€è¦ç™»å½•: {'æ˜¯' if needs_login else 'å¦'}")
        
        # ç­–ç•¥1: Firecrawl (é¦–é€‰)
        result = self._crawl_with_firecrawl(url)
        if result["success"]:
            print("   âœ… Firecrawl æˆåŠŸ")
            return self._save_content(result, url, competitor_name, platform)
        
        # ç­–ç•¥2: Jina Reader (å¤‡é€‰)
        print("   ğŸ”„ é™çº§åˆ° Jina Reader")
        result = self._crawl_with_jina(url)
        if result["success"]:
            print("   âœ… Jina Reader æˆåŠŸ")
            return self._save_content(result, url, competitor_name, platform)
        
        # ç­–ç•¥3: Playwright (å…œåº•) - æš‚æ—¶è·³è¿‡ï¼Œéœ€è¦å®‰è£…æµè§ˆå™¨
        print("   âš ï¸  Playwright æš‚æœªå®ç°")
        
        print("   âŒ æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥")
        return {
            "success": False,
            "error": "æ‰€æœ‰çˆ¬å–ç­–ç•¥éƒ½å¤±è´¥",
            "url": url
        }
    
    def _crawl_with_firecrawl(self, url: str) -> Dict:
        """ä½¿ç”¨ Firecrawl API çˆ¬å–"""
        if not self.firecrawl_key:
            return {"success": False, "error": "æœªé…ç½® FIRECRAWL_API_KEY"}
        
        try:
            from firecrawl import FirecrawlApp
            
            app = FirecrawlApp(api_key=self.firecrawl_key)
            result = app.scrape_url(url)
            
            # Firecrawl v2 è¿”å› Document å¯¹è±¡
            markdown = getattr(result, 'markdown', '')
            metadata = getattr(result, 'metadata', {})
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆ
            if not markdown or len(markdown) < 100:
                return {"success": False, "error": "å†…å®¹å¤ªçŸ­æˆ–ä¸ºç©º"}
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯éªŒè¯é¡µé¢
            if "éªŒè¯" in markdown[:200] or "captcha" in markdown.lower()[:200]:
                return {"success": False, "error": "è§¦å‘éªŒè¯"}
            
            return {
                "success": True,
                "content": markdown,
                "metadata": metadata if isinstance(metadata, dict) else {}
            }
        
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _crawl_with_jina(self, url: str) -> Dict:
        """ä½¿ç”¨ Jina Reader çˆ¬å–"""
        try:
            jina_url = f"https://r.jina.ai/{url}"
            headers = {
                "Accept": "text/markdown",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            response = requests.get(
                jina_url,
                headers=headers,
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            
            content = response.text
            
            # æ£€æŸ¥å†…å®¹æœ‰æ•ˆæ€§
            if not content or len(content) < 100:
                return {"success": False, "error": "å†…å®¹å¤ªçŸ­æˆ–ä¸ºç©º"}
            
            return {
                "success": True,
                "content": content,
                "metadata": {}
            }
        
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def _save_content(
        self,
        crawl_result: Dict,
        url: str,
        competitor_name: str,
        platform: str
    ) -> Dict:
        """ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶"""
        content = crawl_result["content"]
        metadata = crawl_result.get("metadata", {})
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_name = re.sub(r'[^\w\s-]', '_', competitor_name)[:50]
        folder_name = f"{timestamp}_{safe_name}"
        
        # åˆ›å»ºç›®å½•
        save_dir = self.data_dir / folder_name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ Markdown
        content_path = save_dir / "content.md"
        
        # æ·»åŠ å…ƒæ•°æ®å¤´éƒ¨
        header = f"""---
title: {metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')}
url: {url}
platform: {platform}
competitor: {competitor_name}
crawl_time: {datetime.now().isoformat()}
---

"""
        
        with open(content_path, 'w', encoding='utf-8') as f:
            f.write(header + content)
        
        # æå–å¹¶ä¸‹è½½å›¾ç‰‡
        images = self._extract_and_download_images(content, save_dir, url)
        
        # æ›¿æ¢å›¾ç‰‡é“¾æ¥ä¸ºæœ¬åœ°è·¯å¾„
        if images:
            content_with_local_images = self._replace_image_urls(content, images)
            with open(content_path, 'w', encoding='utf-8') as f:
                f.write(header + content_with_local_images)
        
        # è®¡ç®—å†…å®¹å“ˆå¸Œ
        content_hash = hashlib.md5(content.encode()).hexdigest()
        
        print(f"   ğŸ’¾ ä¿å­˜åˆ°: {content_path}")
        print(f"   ğŸ–¼ï¸  å›¾ç‰‡: {len(images)} å¼ ")
        
        return {
            "success": True,
            "content": content,
            "content_path": str(content_path),
            "images": [str(img) for img in images],
            "content_hash": content_hash,
            "metadata": {
                "url": url,
                "platform": platform,
                "competitor": competitor_name,
                "crawl_time": datetime.now().isoformat(),
                "content_length": len(content),
                **metadata
            }
        }
    
    def _extract_and_download_images(
        self,
        content: str,
        save_dir: Path,
        base_url: str
    ) -> List[Path]:
        """æå–å¹¶ä¸‹è½½å›¾ç‰‡"""
        # æå–å›¾ç‰‡ URL
        image_patterns = [
            r'!\[.*?\]\((https?://[^\)]+)\)',  # Markdown æ ¼å¼
            r'(https?://[^\s]+\.(?:jpg|jpeg|png|gif|webp))',  # ç›´æ¥ URL
        ]
        
        image_urls = []
        for pattern in image_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            image_urls.extend(matches)
        
        # å»é‡
        image_urls = list(set(image_urls))
        
        if not image_urls:
            return []
        
        print(f"   ğŸ–¼ï¸  å‘ç° {len(image_urls)} å¼ å›¾ç‰‡")
        
        # ä¸‹è½½å›¾ç‰‡
        downloaded = []
        for i, img_url in enumerate(image_urls[:20], 1):  # æœ€å¤šä¸‹è½½20å¼ 
            try:
                img_path = self._download_image(img_url, save_dir, i, base_url)
                if img_path:
                    downloaded.append(img_path)
            except Exception as e:
                print(f"   âš ï¸  å›¾ç‰‡ {i} ä¸‹è½½å¤±è´¥: {e}")
        
        return downloaded
    
    def _download_image(
        self,
        url: str,
        save_dir: Path,
        index: int,
        base_url: str
    ) -> Optional[Path]:
        """ä¸‹è½½å•å¼ å›¾ç‰‡"""
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": base_url
        }
        
        # å°çº¢ä¹¦å›¾ç‰‡éœ€è¦ç‰¹æ®Š Referer
        if "xiaohongshu.com" in url or "xhscdn.com" in url:
            headers["Referer"] = "https://www.xiaohongshu.com/"
        
        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # ç¡®å®šæ–‡ä»¶æ‰©å±•å
            content_type = response.headers.get('content-type', '')
            ext = '.jpg'
            if 'png' in content_type:
                ext = '.png'
            elif 'gif' in content_type:
                ext = '.gif'
            elif 'webp' in content_type:
                ext = '.webp'
            
            # ä¿å­˜
            img_path = save_dir / f"img_{index:02d}{ext}"
            with open(img_path, 'wb') as f:
                f.write(response.content)
            
            return img_path
        
        except Exception as e:
            return None
    
    def _replace_image_urls(self, content: str, local_images: List[Path]) -> str:
        """æ›¿æ¢å›¾ç‰‡ URL ä¸ºæœ¬åœ°è·¯å¾„"""
        # ç®€å•å®ç°ï¼šæŒ‰é¡ºåºæ›¿æ¢
        # å®é™…åº”è¯¥æ ¹æ® URL åŒ¹é…
        for i, img_path in enumerate(local_images, 1):
            content = re.sub(
                r'!\[(.*?)\]\(https?://[^\)]+\)',
                f'![\\1]({img_path.name})',
                content,
                count=1
            )
        return content
    
    def batch_crawl(
        self,
        urls: List[str],
        competitor_name: str = "Unknown"
    ) -> List[Dict]:
        """æ‰¹é‡çˆ¬å–"""
        results = []
        total = len(urls)
        
        print(f"\nğŸ“¦ æ‰¹é‡çˆ¬å– {total} ä¸ª URL")
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{total}]")
            result = self.crawl(url, competitor_name)
            results.append(result)
        
        success_count = sum(1 for r in results if r.get("success"))
        print(f"\nâœ… å®Œæˆ: {success_count}/{total} æˆåŠŸ")
        
        return results
