"""
æœç´¢å¼•æ“é›†æˆæ¨¡å—
"""
import json
import time
from typing import List, Dict, Optional
import requests
from datetime import datetime, timedelta

from src.config import config
from src.database import SearchCache, SessionLocal


class SearchEngine:
    """æœç´¢å¼•æ“åŸºç±»"""
    
    def __init__(self, use_cache: bool = True):
        self.use_cache = use_cache
    
    def search(self, query: str, num_results: int = 10) -> List[Dict]:
        """æœç´¢æ¥å£"""
        raise NotImplementedError
    
    def _get_cached_results(self, query: str) -> Optional[List[Dict]]:
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        if not self.use_cache:
            return None
        
        db = SessionLocal()
        try:
            cache = db.query(SearchCache).filter(
                SearchCache.query == query,
                SearchCache.expires_at > datetime.utcnow()
            ).first()
            
            if cache:
                cache.hit_count += 1
                db.commit()
                print(f"  ğŸ“¦ ä½¿ç”¨ç¼“å­˜ç»“æœ (å‘½ä¸­æ¬¡æ•°: {cache.hit_count})")
                return cache.results
        finally:
            db.close()
        
        return None
    
    def _save_to_cache(self, query: str, results: List[Dict], engine: str):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        if not self.use_cache:
            return
        
        db = SessionLocal()
        try:
            expires_at = datetime.utcnow() + timedelta(days=config.CACHE_EXPIRY_DAYS)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            cache = db.query(SearchCache).filter(SearchCache.query == query).first()
            if cache:
                cache.results = results
                cache.cached_at = datetime.utcnow()
                cache.expires_at = expires_at
                cache.search_engine = engine
            else:
                cache = SearchCache(
                    query=query,
                    search_engine=engine,
                    results=results,
                    expires_at=expires_at
                )
                db.add(cache)
            
            db.commit()
        finally:
            db.close()


class SerperSearch(SearchEngine):
    """Serper API æœç´¢"""
    
    def __init__(self, api_key: Optional[str] = None, use_cache: bool = True):
        super().__init__(use_cache)
        self.api_key = api_key or config.SERPER_API_KEY
        self.base_url = "https://google.serper.dev/search"
    
    def search(self, query: str, num_results: int = 10, gl: str = "cn", hl: str = "zh-cn") -> List[Dict]:
        """
        ä½¿ç”¨ Serper API æœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            num_results: è¿”å›ç»“æœæ•°é‡
            gl: åœ°åŒºä»£ç  (cn=ä¸­å›½)
            hl: è¯­è¨€ä»£ç  (zh-cn=ç®€ä½“ä¸­æ–‡)
        """
        # æ£€æŸ¥ç¼“å­˜
        cached = self._get_cached_results(query)
        if cached:
            return cached[:num_results]
        
        if not self.api_key:
            print("  âš ï¸  æœªé…ç½® SERPER_API_KEYï¼Œè·³è¿‡")
            return []
        
        headers = {
            "X-API-KEY": self.api_key,
            "Content-Type": "application/json"
        }
        
        payload = {
            "q": query,
            "num": num_results,
            "gl": gl,
            "hl": hl
        }
        
        try:
            response = requests.post(
                self.base_url,
                json=payload,
                headers=headers,
                timeout=config.REQUEST_TIMEOUT
            )
            response.raise_for_status()
            data = response.json()
            
            # æå–æœ‰æœºæœç´¢ç»“æœ
            results = []
            for item in data.get("organic", [])[:num_results]:
                results.append({
                    "title": item.get("title", ""),
                    "url": item.get("link", ""),
                    "snippet": item.get("snippet", ""),
                    "source": "serper"
                })
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_to_cache(query, results, "serper")
            
            return results
        
        except requests.exceptions.RequestException as e:
            print(f"  âŒ Serper æœç´¢å¤±è´¥: {e}")
            return []


class GoogleSearch(SearchEngine):
    """Google Custom Search API"""
    
    def __init__(self, api_key: Optional[str] = None, search_engine_id: Optional[str] = None, use_cache: bool = True):
        super().__init__(use_cache)
        self.api_key = api_key or config.GOOGLE_SEARCH_API_KEY
        self.search_engine_id = search_engine_id or config.GOOGLE_SEARCH_ENGINE_ID
        self.base_url = "https://www.googleapis.com/customsearch/v1"
    
    def search(self, query: str, num_results: int = 10, gl: str = "cn", hl: str = "zh-CN") -> List[Dict]:
        """ä½¿ç”¨ Google Custom Search API"""
        # æ£€æŸ¥ç¼“å­˜
        cached = self._get_cached_results(query)
        if cached:
            return cached[:num_results]
        
        if not self.api_key or not self.search_engine_id:
            print("  âš ï¸  æœªé…ç½® Google Search APIï¼Œè·³è¿‡")
            return []
        
        try:
            results = []
            # Google API ä¸€æ¬¡æœ€å¤šè¿”å›10æ¡
            for start in range(1, min(num_results + 1, 100), 10):
                params = {
                    "key": self.api_key,
                    "cx": self.search_engine_id,
                    "q": query,
                    "num": min(10, num_results - len(results)),
                    "start": start,
                    "gl": gl,
                    "hl": hl
                }
                
                response = requests.get(
                    self.base_url,
                    params=params,
                    timeout=config.REQUEST_TIMEOUT
                )
                response.raise_for_status()
                data = response.json()
                
                for item in data.get("items", []):
                    results.append({
                        "title": item.get("title", ""),
                        "url": item.get("link", ""),
                        "snippet": item.get("snippet", ""),
                        "source": "google"
                    })
                
                if len(results) >= num_results:
                    break
                
                time.sleep(0.5)  # é¿å…é¢‘ç¹è¯·æ±‚
            
            # ä¿å­˜åˆ°ç¼“å­˜
            self._save_to_cache(query, results, "google")
            
            return results[:num_results]
        
        except requests.exceptions.RequestException as e:
            print(f"  âŒ Google æœç´¢å¤±è´¥: {e}")
            return []


class MultiEngineSearch:
    """å¤šå¼•æ“æœç´¢ï¼ˆè‡ªåŠ¨é™çº§ï¼‰"""
    
    def __init__(self, preferred_engine: str = "serper"):
        self.preferred_engine = preferred_engine
        self.engines = {
            "serper": SerperSearch(),
            "google": GoogleSearch()
        }
    
    def search(self, query: str, num_results: int = 10) -> List[Dict]:
        """
        å¤šå¼•æ“æœç´¢ï¼Œè‡ªåŠ¨é™çº§
        ä¼˜å…ˆä½¿ç”¨é…ç½®çš„å¼•æ“ï¼Œå¤±è´¥åå°è¯•å…¶ä»–å¼•æ“
        """
        # å°è¯•é¦–é€‰å¼•æ“
        if self.preferred_engine in self.engines:
            results = self.engines[self.preferred_engine].search(query, num_results)
            if results:
                return results
        
        # é™çº§ï¼šå°è¯•å…¶ä»–å¼•æ“
        for engine_name, engine in self.engines.items():
            if engine_name == self.preferred_engine:
                continue
            
            print(f"  ğŸ”„ é™çº§åˆ° {engine_name}")
            results = engine.search(query, num_results)
            if results:
                return results
        
        return []
    
    def batch_search(self, queries: List[str], num_results: int = 10) -> Dict[str, List[Dict]]:
        """æ‰¹é‡æœç´¢"""
        results = {}
        for query in queries:
            print(f"ğŸ” æœç´¢: {query}")
            results[query] = self.search(query, num_results)
            time.sleep(1)  # é¿å…é¢‘ç¹è¯·æ±‚
        return results
