"""
ç«å“å‘ç°å’Œæå–æ¨¡å—
"""
import json
import re
from typing import List, Dict, Tuple, Optional
from datetime import datetime
from fuzzywuzzy import fuzz
from openai import OpenAI

from src.config import config
from src.database import DiscoveryTask, Competitor, DataSource, SessionLocal
from src.discovery.search_engine import MultiEngineSearch


class CompetitorDiscoverer:
    """ç«å“å‘ç°å™¨"""
    
    def __init__(self, search_engine: Optional[str] = None):
        self.search_engine = MultiEngineSearch(
            preferred_engine=search_engine or config.DEFAULT_SEARCH_ENGINE
        )
        self.client = OpenAI(api_key=config.OPENAI_API_KEY)
    
    def discover(
        self,
        topic: str,
        market: str = "ä¸­å›½",
        target_count: int = 5,
        depth: str = "standard"
    ) -> Dict:
        """
        å‘ç°ç«å“
        
        Args:
            topic: è°ƒç ”ä¸»é¢˜ï¼Œå¦‚ "AIå†™ä½œåŠ©æ‰‹"
            market: ç›®æ ‡å¸‚åœº
            target_count: ç›®æ ‡ç«å“æ•°é‡
            depth: æœç´¢æ·±åº¦ quick/standard/deep
        
        Returns:
            {
                "competitors": [ç«å“åˆ—è¡¨],
                "task_id": ä»»åŠ¡ID
            }
        """
        print(f"\nğŸš€ å¼€å§‹ç«å“å‘ç°: {topic}")
        print(f"ğŸ“ ç›®æ ‡å¸‚åœº: {market} | ç›®æ ‡æ•°é‡: {target_count} | æ·±åº¦: {depth}")
        
        # åˆ›å»ºå‘ç°ä»»åŠ¡
        task = self._create_task(topic, market, target_count, depth)
        
        try:
            # é˜¶æ®µ1ï¼šå‘ç°ç«å“
            print("\n" + "="*60)
            print("é˜¶æ®µ1: ç«å“å‘ç°")
            print("="*60)
            competitors = self._discover_competitors(topic, market, target_count, depth)
            task.competitors_found = len(competitors)
            task.progress = 50
            self._update_task(task)
            
            # é˜¶æ®µ2ï¼šæœç´¢æ•°æ®æº
            print("\n" + "="*60)
            print("é˜¶æ®µ2: æ•°æ®æºæœç´¢")
            print("="*60)
            for comp in competitors:
                sources = self._discover_data_sources(comp["name"], topic)
                comp["data_sources"] = sources
            
            total_sources = sum(len(c.get("data_sources", [])) for c in competitors)
            task.sources_found = total_sources
            task.progress = 100
            task.status = "completed"
            task.completed_at = datetime.utcnow()
            task.result_data = {"competitors": competitors}
            self._update_task(task)
            
            # ä¿å­˜ç«å“åˆ°æ•°æ®åº“
            self._save_competitors(task.id, competitors)
            
            print("\n" + "="*60)
            print(f"âœ… å‘ç°å®Œæˆ!")
            print(f"ğŸ“Š æ‰¾åˆ° {len(competitors)} ä¸ªç«å“ï¼Œå…± {total_sources} ä¸ªæ•°æ®æº")
            print("="*60)
            
            return {
                "task_id": task.id,
                "competitors": competitors,
                "total_sources": total_sources
            }
        
        except Exception as e:
            task.status = "failed"
            task.result_data = {"error": str(e)}
            self._update_task(task)
            raise
    
    def _discover_competitors(
        self,
        topic: str,
        market: str,
        target_count: int,
        depth: str
    ) -> List[Dict]:
        """å‘ç°ç«å“åç§°"""
        # æ„é€ æœç´¢æŸ¥è¯¢
        queries = self._build_discovery_queries(topic, market, depth)
        
        print(f"ğŸ“ ç”Ÿæˆ {len(queries)} ä¸ªæœç´¢æŸ¥è¯¢")
        
        # æ‰¹é‡æœç´¢
        search_results = self.search_engine.batch_search(queries, num_results=10)
        
        # æå–ç«å“
        all_competitors = []
        
        for query, results in search_results.items():
            if not results:
                continue
            
            print(f"\n  åˆ†ææŸ¥è¯¢: {query} ({len(results)} æ¡ç»“æœ)")
            
            # ä»æœç´¢ç»“æœä¸­æå–ç«å“
            competitors = self._extract_competitors_from_results(
                topic, results, max_competitors=10
            )
            
            all_competitors.extend(competitors)
        
        # å»é‡å’Œåˆå¹¶
        unique_competitors = self._deduplicate_competitors(all_competitors)
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºï¼Œå–å‰Nä¸ª
        unique_competitors.sort(key=lambda x: x["confidence"], reverse=True)
        final_competitors = unique_competitors[:target_count]
        
        print(f"\nğŸ“Š å»é‡åå¾—åˆ° {len(unique_competitors)} ä¸ªç«å“")
        print(f"âœ… é€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„ {len(final_competitors)} ä¸ª")
        
        return final_competitors
    
    def _build_discovery_queries(self, topic: str, market: str, depth: str) -> List[str]:
        """æ„é€ æœç´¢æŸ¥è¯¢"""
        queries = []
        
        if depth in ["quick", "standard", "deep"]:
            queries.extend([
                f"{topic} ç«å“",
                f"{topic} å¯¹æ¯”",
                f"{topic} æœ‰å“ªäº›",
            ])
        
        if depth in ["standard", "deep"]:
            queries.extend([
                f"best {topic} alternatives",
                f"{topic} vs",
                f"{topic} æ’è¡Œæ¦œ",
            ])
        
        if depth == "deep":
            queries.extend([
                f"{topic} æ¨è",
                f"{topic} è¯„æµ‹",
                f"top {topic} tools",
            ])
        
        return queries
    
    def _extract_competitors_from_results(
        self,
        topic: str,
        search_results: List[Dict],
        max_competitors: int = 10
    ) -> List[Dict]:
        """ä½¿ç”¨ LLM ä»æœç´¢ç»“æœä¸­æå–ç«å“"""
        # åˆå¹¶æœç´¢ç»“æœæ–‡æœ¬
        context = ""
        for i, result in enumerate(search_results[:5], 1):  # åªå–å‰5æ¡
            context += f"{i}. {result['title']}\n{result['snippet']}\n\n"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¸‚åœºç ”ç©¶åˆ†æå¸ˆã€‚è¯·ä»ä»¥ä¸‹æœç´¢ç»“æœä¸­æå–æ‰€æœ‰æåˆ°çš„ "{topic}" ç›¸å…³äº§å“/å·¥å…·çš„åç§°ã€‚

æœç´¢ç»“æœï¼š
{context}

è¦æ±‚ï¼š
1. åªæå–æ˜ç¡®æåˆ°çš„äº§å“åç§°ï¼Œä¸è¦è‡†æµ‹
2. æ’é™¤é€šç”¨åè¯ï¼ˆå¦‚"AIå·¥å…·"ã€"è½¯ä»¶"ç­‰ï¼‰
3. æ¯ä¸ªäº§å“ç»™å‡ºç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0-1ï¼‰ï¼ŒåŸºäºå®ƒåœ¨å†…å®¹ä¸­çš„æåŠé¢‘ç‡å’Œç›¸å…³æ€§
4. æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œæœ€å¤šè¿”å›{max_competitors}ä¸ªäº§å“

è¾“å‡ºæ ¼å¼ï¼ˆçº¯JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼‰ï¼š
{{
    "competitors": [
        {{"name": "äº§å“å", "confidence": 0.95, "reason": "åœ¨æœç´¢ç»“æœä¸­å¤šæ¬¡æ˜ç¡®æåˆ°"}},
        {{"name": "äº§å“å2", "confidence": 0.80, "reason": "åœ¨æ ‡é¢˜ä¸­æåˆ°"}}
    ]
}}"""
        
        try:
            response = self.client.chat.completions.create(
                model=config.DEFAULT_LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content
            data = json.loads(content)
            
            competitors = data.get("competitors", [])
            print(f"    æå–åˆ° {len(competitors)} ä¸ªç«å“")
            
            return competitors
        
        except Exception as e:
            print(f"    âŒ LLM æå–å¤±è´¥: {e}")
            return []
    
    def _deduplicate_competitors(self, competitors: List[Dict]) -> List[Dict]:
        """å»é‡å’Œåˆå¹¶ç«å“"""
        unique = []
        
        for comp in competitors:
            is_duplicate = False
            comp_name = comp["name"].lower().strip()
            
            for existing in unique:
                existing_name = existing["name"].lower().strip()
                
                # ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…åˆ¤æ–­ç›¸ä¼¼åº¦
                similarity = fuzz.ratio(comp_name, existing_name)
                
                if similarity > 85:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    # åˆå¹¶ç½®ä¿¡åº¦ï¼ˆå–æœ€é«˜å€¼ï¼‰
                    existing["confidence"] = max(
                        existing.get("confidence", 0),
                        comp.get("confidence", 0)
                    )
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique.append(comp)
        
        return unique
    
    def _discover_data_sources(self, competitor_name: str, topic: str) -> List[Dict]:
        """ä¸ºå•ä¸ªç«å“å‘ç°æ•°æ®æº"""
        print(f"\n  ğŸ” æœç´¢ {competitor_name} çš„æ•°æ®æº...")
        
        # æ•°æ®æºæœç´¢æ¨¡æ¿
        source_queries = {
            "å®˜ç½‘": [
                f"{competitor_name} å®˜ç½‘",
                f"{competitor_name} official website"
            ],
            "äº§å“åŠŸèƒ½": [
                f"{competitor_name} features",
                f"{competitor_name} åŠŸèƒ½ä»‹ç»"
            ],
            "å®šä»·": [
                f"{competitor_name} pricing",
                f"{competitor_name} ä»·æ ¼"
            ],
            "ç”¨æˆ·è¯„ä»·": [
                f"{competitor_name} è¯„ä»· site:xiaohongshu.com",
                f"{competitor_name} æ€ä¹ˆæ · site:zhihu.com"
            ]
        }
        
        all_sources = []
        
        for source_type, queries in source_queries.items():
            # åªæœç´¢ç¬¬ä¸€ä¸ªæŸ¥è¯¢ï¼ˆèŠ‚çœæˆæœ¬ï¼‰
            query = queries[0]
            results = self.search_engine.search(query, num_results=3)
            
            if results:
                for result in results[:2]:  # æ¯ç§ç±»å‹å–å‰2ä¸ª
                    all_sources.append({
                        "type": source_type,
                        "url": result["url"],
                        "title": result["title"],
                        "priority": self._get_priority(source_type),
                        "quality_score": 0.8  # é»˜è®¤è¯„åˆ†
                    })
        
        print(f"    æ‰¾åˆ° {len(all_sources)} ä¸ªæ•°æ®æº")
        return all_sources
    
    def _get_priority(self, source_type: str) -> int:
        """è·å–æ•°æ®æºä¼˜å…ˆçº§"""
        priority_map = {
            "å®˜ç½‘": 1,
            "äº§å“åŠŸèƒ½": 1,
            "å®šä»·": 1,
            "ç”¨æˆ·è¯„ä»·": 2,
            "ç”µå•†": 2,
            "åšå®¢æ–‡ç« ": 3,
            "å…¶ä»–": 4
        }
        return priority_map.get(source_type, 4)
    
    def _create_task(self, topic: str, market: str, target_count: int, depth: str) -> DiscoveryTask:
        """åˆ›å»ºå‘ç°ä»»åŠ¡"""
        db = SessionLocal()
        try:
            task = DiscoveryTask(
                topic=topic,
                market=market,
                target_count=target_count,
                search_depth=depth,
                status="processing"
            )
            db.add(task)
            db.commit()
            db.refresh(task)
            return task
        finally:
            db.close()
    
    def _update_task(self, task: DiscoveryTask):
        """æ›´æ–°ä»»åŠ¡"""
        db = SessionLocal()
        try:
            db.merge(task)
            db.commit()
        finally:
            db.close()
    
    def _save_competitors(self, task_id: int, competitors: List[Dict]):
        """ä¿å­˜ç«å“åˆ°æ•°æ®åº“"""
        db = SessionLocal()
        try:
            for comp_data in competitors:
                # åˆ›å»ºç«å“
                competitor = Competitor(
                    name=comp_data["name"],
                    discovery_task_id=task_id,
                    confidence=comp_data.get("confidence", 0.5),
                    status="active"
                )
                db.add(competitor)
                db.flush()  # è·å– ID
                
                # åˆ›å»ºæ•°æ®æº
                for source_data in comp_data.get("data_sources", []):
                    data_source = DataSource(
                        competitor_id=competitor.id,
                        source_type=source_data["type"],
                        url=source_data["url"],
                        priority=source_data["priority"],
                        quality_score=source_data.get("quality_score", 0.8),
                        auto_discovered=True,
                        status="active"
                    )
                    db.add(data_source)
            
            db.commit()
        finally:
            db.close()
